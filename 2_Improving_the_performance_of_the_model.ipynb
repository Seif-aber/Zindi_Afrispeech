{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT NOTE :** This notebook is very similar to the first. What's new is the data augmentation section.\n",
        "\n"
      ],
      "metadata": {
        "id": "095mDfKWXImw"
      },
      "id": "095mDfKWXImw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyrights to Sanchit Gandhi [Fine-Tune Whisper For Multilingual ASR with ðŸ¤— Transformers\n",
        "](https://huggingface.co/blog/fine-tune-whisper). \n",
        "\n",
        "For more details, please visit the link."
      ],
      "metadata": {
        "id": "PNA5aFpu0kV1"
      },
      "id": "PNA5aFpu0kV1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Colab, we present a step-by-step guide on how to fine-tune Whisper \n",
        "for any multilingual ASR dataset using Hugging Face ðŸ¤— Transformers. This is a \n",
        "more \"hands-on\" version of the accompanying [blog post](https://huggingface.co/blog/fine-tune-whisper). \n",
        "For a more in-depth explanation of Whisper, the Common Voice dataset and the theory behind fine-tuning, the reader is advised to refer to the blog post."
      ],
      "metadata": {
        "id": "wcz8xkgi-dKN"
      },
      "id": "wcz8xkgi-dKN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's necessary to have a GPU that have enough memory to train this model (like A100 of Colab Pro.)"
      ],
      "metadata": {
        "id": "0X9IeSYTaYqf"
      },
      "id": "0X9IeSYTaYqf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Environment\n"
      ],
      "metadata": {
        "id": "QCwaBZ2U2UCx"
      },
      "id": "QCwaBZ2U2UCx"
    },
    {
      "cell_type": "code",
      "source": [
        "# We check the GPU device info\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "kV7BJkD553Ja",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:33:49.775878Z",
          "iopub.execute_input": "2023-05-30T15:33:49.776452Z",
          "iopub.status.idle": "2023-05-30T15:33:49.845980Z",
          "shell.execute_reply.started": "2023-05-30T15:33:49.776420Z",
          "shell.execute_reply": "2023-05-30T15:33:49.844928Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "kV7BJkD553Ja"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll employ several popular Python packages to fine-tune the Whisper model.\n",
        "We'll use `datasets` to download and prepare our training data and \n",
        "`transformers` to load and train our Whisper model. We'll also require\n",
        "the `soundfile` package to pre-process audio files, `evaluate` and `jiwer` to\n",
        "assess the performance of our model. Finally, we'll\n",
        "use `gradio` to build a flashy demo of our fine-tuned model."
      ],
      "metadata": {
        "id": "K1ChErUk6hUM"
      },
      "id": "K1ChErUk6hUM"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets accelerate\n",
        "!pip install transformers \n",
        "!pip install librosa\n",
        "!pip install evaluate \n",
        "!pip install jiwer\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "5f5534ec-9055-4410-ad83-a97484327ef5",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:34:09.382683Z",
          "iopub.execute_input": "2023-05-30T15:34:09.383043Z",
          "iopub.status.idle": "2023-05-30T15:35:22.453832Z",
          "shell.execute_reply.started": "2023-05-30T15:34:09.383013Z",
          "shell.execute_reply": "2023-05-30T15:35:22.452461Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "5f5534ec-9055-4410-ad83-a97484327ef5"
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use wandb to visualize the training progress\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "Y90Rgq8N3gAC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Y90Rgq8N3gAC"
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"Seyfelislem/afripspeech_data_aug\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-30T15:44:36.511073Z",
          "iopub.execute_input": "2023-05-30T15:44:36.511495Z",
          "iopub.status.idle": "2023-05-30T15:45:18.447817Z",
          "shell.execute_reply.started": "2023-05-30T15:44:36.511463Z",
          "shell.execute_reply": "2023-05-30T15:45:18.438660Z"
        },
        "trusted": true,
        "id": "6GIWuPHdWEvm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6GIWuPHdWEvm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we want to upload your training checkpoints directly to the [ðŸ¤— Hub](https://huggingface.co/) while training. The [ðŸ¤— Hub](https://huggingface.co/) has integrated version control so you can be sure that no model checkpoint is getting lost during training. \n",
        "\n",
        "To do so we have to store our authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!)"
      ],
      "metadata": {
        "id": "_NTdj9NbFxxM"
      },
      "id": "_NTdj9NbFxxM"
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "60a8dbb5-d13e-46c0-9fe8-4293a29b7c7c"
      },
      "execution_count": null,
      "outputs": [],
      "id": "60a8dbb5-d13e-46c0-9fe8-4293a29b7c7c"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_name_or_path = \"Seyfelislem/afrispeech_large_A100\"\n",
        "language = \"English\"\n",
        "language_abbr = \"en\"\n",
        "task = \"transcribe\"\n",
        "dataset_name = \"tobiolatunji/afrispeech-200\""
      ],
      "metadata": {
        "id": "707bb451-c61b-4b2a-bd3b-6748b535a850",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:37:56.038618Z",
          "iopub.execute_input": "2023-05-30T15:37:56.039718Z",
          "iopub.status.idle": "2023-05-30T15:37:56.045551Z",
          "shell.execute_reply.started": "2023-05-30T15:37:56.039666Z",
          "shell.execute_reply": "2023-05-30T15:37:56.044456Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "707bb451-c61b-4b2a-bd3b-6748b535a850"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "qCjRhrVfDeoG"
      },
      "id": "qCjRhrVfDeoG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load our dataset with the streaming mode. For more details check this [notebook](https://github.com/bofenghuang/community-events/blob/bh/whisper-fine-tuning-event/fine-tune-whisper-streaming.ipynb)."
      ],
      "metadata": {
        "id": "pyHfiOqHEar0"
      },
      "id": "pyHfiOqHEar0"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"tobiolatunji/afrispeech-200\",\"all\", streaming=True\n",
        ")"
      ],
      "metadata": {
        "id": "282c56df-d4e2-4de7-9c64-4fd73036f6f9",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:37:58.652783Z",
          "iopub.execute_input": "2023-05-30T15:37:58.655755Z",
          "iopub.status.idle": "2023-05-30T15:38:04.622483Z",
          "shell.execute_reply.started": "2023-05-30T15:37:58.655714Z",
          "shell.execute_reply": "2023-05-30T15:38:04.621447Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "282c56df-d4e2-4de7-9c64-4fd73036f6f9"
    },
    {
      "cell_type": "code",
      "source": [
        "# we remove some columns that won't be useful \n",
        "dataset = dataset.remove_columns([\"speaker_id\",\"path\",\"audio_id\",\"age_group\",\"gender\",\"accent\",\"domain\",\"country\",\"duration\"])\n",
        "dataset = dataset.rename_column(\"transcript\",\"sentence\")\n",
        "dataset[\"train\"].features"
      ],
      "metadata": {
        "id": "82683dd4-5be7-44e3-a4f5-b9962450867c",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:38:26.681654Z",
          "iopub.execute_input": "2023-05-30T15:38:26.682740Z",
          "iopub.status.idle": "2023-05-30T15:38:26.694976Z",
          "shell.execute_reply.started": "2023-05-30T15:38:26.682704Z",
          "shell.execute_reply": "2023-05-30T15:38:26.693835Z"
        },
        "trusted": true,
        "outputId": "aee575c0-ac39-4b2f-ef81-8346d62ce0b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'audio': Audio(sampling_rate=44100, mono=True, decode=True, id=None),\n 'sentence': Value(dtype='string', id=None)}"
          },
          "metadata": {}
        }
      ],
      "id": "82683dd4-5be7-44e3-a4f5-b9962450867c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to _downsample_ audio files to \n",
        "16kHz prior to passing it to the Whisper feature extractor. \n",
        "\n",
        "We'll set the audio inputs to the correct sampling rate using dataset's \n",
        "[`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column)\n",
        "method. This operation does not change the audio in-place, \n",
        "but rather signals to `datasets` to resample audio samples _on the fly_ the \n",
        "first time that they are loaded:"
      ],
      "metadata": {
        "id": "639SnXmz8uEg"
      },
      "id": "639SnXmz8uEg"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "wKFs8ZZ778ws",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:38:29.433137Z",
          "iopub.execute_input": "2023-05-30T15:38:29.433991Z",
          "iopub.status.idle": "2023-05-30T15:38:29.440467Z",
          "shell.execute_reply.started": "2023-05-30T15:38:29.433952Z",
          "shell.execute_reply": "2023-05-30T15:38:29.439562Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "wKFs8ZZ778ws"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data augmentation"
      ],
      "metadata": {
        "id": "hXEENROVbfJk"
      },
      "id": "hXEENROVbfJk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation refers to the process of artificially expanding a given dataset by applying various transformations or perturbations to the existing audio samples. It helps in creating additional diverse training examples, which can improve the generalization capabilities of ASR models. [Audiomentations]((https://github.com/iver56/audiomentations)) is a Python library specifically designed for audio data augmentation. It provides a wide range of audio transformations that can be applied to speech signals, including time stretching, pitch shifting, background noise addition, and more. By leveraging audiomentations, you can easily integrate data augmentation into your ASR pipeline.\n"
      ],
      "metadata": {
        "id": "ewY_Rhm3cZzB"
      },
      "id": "ewY_Rhm3cZzB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IONVCTo7b5I6"
      },
      "outputs": [],
      "source": [
        "!pip install audiomentations"
      ],
      "id": "IONVCTo7b5I6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj7v8qXmb5JC"
      },
      "outputs": [],
      "source": [
        "augmented_dataset = dataset"
      ],
      "id": "rj7v8qXmb5JC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsW90wB6b5JC"
      },
      "outputs": [],
      "source": [
        "from audiomentations import (\n",
        "    \n",
        "    Compose,\n",
        "    Gain,\n",
        "    OneOf,\n",
        "    PitchShift,\n",
        "    PolarityInversion,\n",
        "    TimeStretch,\n",
        ")\n",
        "\n",
        "# define augmentation\n",
        "augmentation = Compose(\n",
        "    [\n",
        "        TimeStretch(min_rate=0.9, max_rate=1.1, p=0.2, leave_length_unchanged=False),\n",
        "        Gain(min_gain_in_db=-6, max_gain_in_db=6, p=0.1),\n",
        "        PitchShift(min_semitones=-4, max_semitones=4, p=0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "def augment_dataset(batch):\n",
        "    # load audio data\n",
        "    sample = batch['audio']\n",
        "\n",
        "    # apply augmentation\n",
        "    augmented_waveform = augmentation(sample[\"array\"], sample_rate=sample[\"sampling_rate\"])\n",
        "    batch['audio'][\"array\"] = augmented_waveform.astype(float)\n",
        "\n",
        "    return batch"
      ],
      "id": "YsW90wB6b5JC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idqXQAtFb5JD"
      },
      "outputs": [],
      "source": [
        "# we apply augmentation so we create new data\n",
        "augmented_dataset[\"train\"] = augmented_dataset[\"train\"].map(\n",
        "    augment_dataset\n",
        ")"
      ],
      "id": "idqXQAtFb5JD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooh62gbkb5JD"
      },
      "outputs": [],
      "source": [
        "# We skip the first 500 samples since we will use the original ones for evaluation\n",
        "augmented_dataset[\"train\"] = augmented_dataset[\"train\"].skip(500)"
      ],
      "id": "ooh62gbkb5JD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81T_HFIxb5JD"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "# We concatenate the two datasets\n",
        "ds = concatenate_datasets([dataset[\"train\"], augmented_dataset[\"train\"]])"
      ],
      "id": "81T_HFIxb5JD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Feature Extractor, Tokenizer and Data"
      ],
      "metadata": {
        "id": "7HJLGfk-8A4e"
      },
      "id": "7HJLGfk-8A4e"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large-v2\")\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large-v2\", language=language, task=task)\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\", language=language, task=task)"
      ],
      "metadata": {
        "id": "63df2617-8bf4-41d2-ba71-e3e8107002bf",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:38:31.212046Z",
          "iopub.execute_input": "2023-05-30T15:38:31.212743Z",
          "iopub.status.idle": "2023-05-30T15:38:38.319095Z",
          "shell.execute_reply.started": "2023-05-30T15:38:31.212708Z",
          "shell.execute_reply": "2023-05-30T15:38:38.318068Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "63df2617-8bf4-41d2-ba71-e3e8107002bf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can write a function to prepare our data ready for the model:\n",
        "1. We load and resample the audio data by calling `batch[\"audio\"]`. As explained above, ðŸ¤— Datasets performs any necessary resampling operations on the fly.\n",
        "2. We use the feature extractor to compute the log-Mel spectrogram input features from our 1-dimensional audio array.\n",
        "3. We encode the transcriptions to label ids through the use of the tokenizer."
      ],
      "metadata": {
        "id": "AU-LEpJk86LW"
      },
      "id": "AU-LEpJk86LW"
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute input length\n",
        "    batch[\"input_length\"] = len(batch[\"audio\"])\n",
        "\n",
        "    # compute log-Mel input features from input audio array \n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids \n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "\n",
        "    # compute labels length\n",
        "    batch[\"labels_length\"] = len(batch[\"labels\"])\n",
        "    return batch"
      ],
      "metadata": {
        "id": "b52b560f-e6d7-420f-8ae7-7f48964e2192",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:39:47.221732Z",
          "iopub.execute_input": "2023-05-30T15:39:47.222540Z",
          "iopub.status.idle": "2023-05-30T15:39:47.231831Z",
          "shell.execute_reply.started": "2023-05-30T15:39:47.222479Z",
          "shell.execute_reply": "2023-05-30T15:39:47.230570Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "b52b560f-e6d7-420f-8ae7-7f48964e2192"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Â Load a Pre-Trained Checkpoint"
      ],
      "metadata": {
        "id": "daf2a825-6d9f-4a23-b145-c37c0039075b"
      },
      "id": "daf2a825-6d9f-4a23-b145-c37c0039075b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load the pre-trained Whisper `large-v2` checkpoint. Again, this \n",
        "is trivial through use of ðŸ¤— Transformers!"
      ],
      "metadata": {
        "id": "437a97fa-4864-476b-8abc-f28b8166cfa5"
      },
      "id": "437a97fa-4864-476b-8abc-f28b8166cfa5"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "id": "e6f5ab46-f817-4c31-b61a-c867f54689ac",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:39:51.158091Z",
          "iopub.execute_input": "2023-05-30T15:39:51.158471Z",
          "iopub.status.idle": "2023-05-30T15:40:44.608781Z",
          "shell.execute_reply.started": "2023-05-30T15:39:51.158440Z",
          "shell.execute_reply": "2023-05-30T15:40:44.606115Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "e6f5ab46-f817-4c31-b61a-c867f54689ac"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Override generation arguments - no tokens are forced as decoder outputs (see [`forced_decoder_ids`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.forced_decoder_ids)), no tokens are suppressed during generation (see [`suppress_tokens`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.suppress_tokens)). Set use_cache to False since we're using gradient checkpointing, and the two are incompatible:"
      ],
      "metadata": {
        "id": "jrMYeN4dHXZ0"
      },
      "id": "jrMYeN4dHXZ0"
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "36b44853-395d-476c-8c87-269c44ddafb7",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:40:44.617584Z",
          "iopub.execute_input": "2023-05-30T15:40:44.621760Z",
          "iopub.status.idle": "2023-05-30T15:40:44.632668Z",
          "shell.execute_reply.started": "2023-05-30T15:40:44.621696Z",
          "shell.execute_reply": "2023-05-30T15:40:44.630966Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "36b44853-395d-476c-8c87-269c44ddafb7"
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_DURATION_IN_SECONDS = 30.0\n",
        "max_input_length = MAX_DURATION_IN_SECONDS * 16000\n",
        "\n",
        "def filter_inputs(input_length):\n",
        "    \"\"\"Filter inputs with zero input length or longer than 30s\"\"\"\n",
        "    return 0 < input_length < max_input_length\n",
        "\n",
        "max_label_length = model.config.max_length\n",
        "\n",
        "def filter_labels(labels_length):\n",
        "    \"\"\"Filter label sequences longer than max length (448)\"\"\"\n",
        "    return labels_length < max_label_length"
      ],
      "metadata": {
        "id": "ad3092b5-aafb-46b9-a9fd-1db964bc4155",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:40:44.639885Z",
          "iopub.execute_input": "2023-05-30T15:40:44.641086Z",
          "iopub.status.idle": "2023-05-30T15:40:44.656864Z",
          "shell.execute_reply.started": "2023-05-30T15:40:44.640996Z",
          "shell.execute_reply": "2023-05-30T15:40:44.653704Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "ad3092b5-aafb-46b9-a9fd-1db964bc4155"
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-process\n",
        "dataset = dataset.map(prepare_dataset)\n",
        "# filter by audio length\n",
        "dataset = dataset.filter(filter_inputs, input_columns=[\"input_length\"])\n",
        "# filter by label length\n",
        "dataset = dataset.filter(filter_labels, input_columns=[\"labels_length\"])"
      ],
      "metadata": {
        "id": "10da7fe2-4078-4fdf-920c-dfab50c1ac42",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:40:44.660091Z",
          "iopub.execute_input": "2023-05-30T15:40:44.661977Z",
          "iopub.status.idle": "2023-05-30T15:40:44.679312Z",
          "shell.execute_reply.started": "2023-05-30T15:40:44.661916Z",
          "shell.execute_reply": "2023-05-30T15:40:44.677417Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "10da7fe2-4078-4fdf-920c-dfab50c1ac42"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation"
      ],
      "metadata": {
        "id": "USpS80FTGpv_"
      },
      "id": "USpS80FTGpv_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've prepared our data, we're ready to dive into the training pipeline. \n",
        "The [ðŸ¤— Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer)\n",
        "will do much of the heavy lifting for us. All we have to do is:\n",
        "\n",
        "- Define a data collator: the data collator takes our pre-processed data and prepares PyTorch tensors ready for the model.\n",
        "\n",
        "- Evaluation metrics: during evaluation, we want to evaluate the model using the [word error rate (WER)](https://huggingface.co/metrics/wer) metric. We need to define a `compute_metrics` function that handles this computation.\n",
        "\n",
        "- Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n",
        "\n",
        "- Define the training configuration: this will be used by the ðŸ¤— Trainer to define the training schedule."
      ],
      "metadata": {
        "id": "1-lp_-nXGrw7"
      },
      "id": "1-lp_-nXGrw7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Data Collator"
      ],
      "metadata": {
        "id": "39IKVpUAG0LR"
      },
      "id": "39IKVpUAG0LR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data collator for a sequence-to-sequence speech model is unique in the sense that it \n",
        "treats the `input_features` and `labels` independently: the  `input_features` must be \n",
        "handled by the feature extractor and the `labels` by the tokenizer.\n",
        "\n",
        "The `input_features` are already padded to 30s and converted to a log-Mel spectrogram \n",
        "of fixed dimension by action of the feature extractor, so all we have to do is convert the `input_features`\n",
        "to batched PyTorch tensors. We do this using the feature extractor's `.pad` method with `return_tensors=pt`.\n",
        "\n",
        "The `labels` on the other hand are un-padded. We first pad the sequences\n",
        "to the maximum length in the batch using the tokenizer's `.pad` method. The padding tokens \n",
        "are then replaced by `-100` so that these tokens are **not** taken into account when \n",
        "computing the loss. We then cut the BOS token from the start of the label sequence as we \n",
        "append it later during training.\n",
        "\n",
        "We can leverage the `WhisperProcessor` we defined earlier to perform both the \n",
        "feature extractor and the tokenizer operations:"
      ],
      "metadata": {
        "id": "I2pXURfEGr2V"
      },
      "id": "I2pXURfEGr2V"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "c15c7a1b-49b9-45c1-a80e-3d7c887793c5",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:40:49.978495Z",
          "iopub.execute_input": "2023-05-30T15:40:49.979578Z",
          "iopub.status.idle": "2023-05-30T15:40:49.991204Z",
          "shell.execute_reply.started": "2023-05-30T15:40:49.979526Z",
          "shell.execute_reply": "2023-05-30T15:40:49.990096Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "c15c7a1b-49b9-45c1-a80e-3d7c887793c5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's initialise the data collator we've just defined:"
      ],
      "metadata": {
        "id": "-xZhbZNyG5ve"
      },
      "id": "-xZhbZNyG5ve"
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "891332b0-47e5-4fec-aff6-a47d67159087",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:41:16.568364Z",
          "iopub.execute_input": "2023-05-30T15:41:16.569370Z",
          "iopub.status.idle": "2023-05-30T15:41:16.574257Z",
          "shell.execute_reply.started": "2023-05-30T15:41:16.569336Z",
          "shell.execute_reply": "2023-05-30T15:41:16.572986Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "891332b0-47e5-4fec-aff6-a47d67159087"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Metrics"
      ],
      "metadata": {
        "id": "5TP7ggZZG8Mz"
      },
      "id": "5TP7ggZZG8Mz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the word error rate (WER) metric, the 'de-facto' metric for assessing \n",
        "ASR systems. For more information, refer to the WER [docs](https://huggingface.co/metrics/wer). We'll load the WER metric from ðŸ¤— Evaluate:"
      ],
      "metadata": {
        "id": "9F8ak4bYG-Ms"
      },
      "id": "9F8ak4bYG-Ms"
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "be318294-31c2-4487-94c0-464c8422cf15",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:41:18.819327Z",
          "iopub.execute_input": "2023-05-30T15:41:18.820033Z",
          "iopub.status.idle": "2023-05-30T15:41:22.516566Z",
          "shell.execute_reply.started": "2023-05-30T15:41:18.819996Z",
          "shell.execute_reply": "2023-05-30T15:41:22.515372Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "4af7b771fd2a4e2aa62ec86e2b033aa9"
          ]
        },
        "outputId": "4381487f-79f6-4ed9-9b6d-ea8426118a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4af7b771fd2a4e2aa62ec86e2b033aa9"
            }
          },
          "metadata": {}
        }
      ],
      "id": "be318294-31c2-4487-94c0-464c8422cf15"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then simply have to define a function that takes our model \n",
        "predictions and returns the WER metric. This function, called\n",
        "`compute_metrics`, first replaces `-100` with the `pad_token_id`\n",
        "in the `label_ids` (undoing the step we applied in the \n",
        "data collator to ignore padded tokens correctly in the loss).\n",
        "It then decodes the predicted and label ids to strings. Finally,\n",
        "it computes the WER between the predictions and reference labels:"
      ],
      "metadata": {
        "id": "onT_h6GUHBEN"
      },
      "id": "onT_h6GUHBEN"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "7f64be8b-ce51-4765-a2ca-720ec3dbb9ae",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:41:24.070205Z",
          "iopub.execute_input": "2023-05-30T15:41:24.070604Z",
          "iopub.status.idle": "2023-05-30T15:41:24.080957Z",
          "shell.execute_reply.started": "2023-05-30T15:41:24.070568Z",
          "shell.execute_reply": "2023-05-30T15:41:24.077959Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "7f64be8b-ce51-4765-a2ca-720ec3dbb9ae"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Training Configuration"
      ],
      "metadata": {
        "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06"
      },
      "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the final step, we define all the parameters related to training. For more detail on the training arguments, refer to the Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)."
      ],
      "metadata": {
        "id": "c21af1e9-0188-4134-ac82-defc7bdcc436"
      },
      "id": "c21af1e9-0188-4134-ac82-defc7bdcc436"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Training Configuration"
      ],
      "metadata": {
        "id": "Vp8wyibRH731"
      },
      "id": "Vp8wyibRH731"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the final step, we define all the parameters related to training. For more detail on the training arguments, refer to the Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)."
      ],
      "metadata": {
        "id": "XXQRXFCXH731"
      },
      "id": "XXQRXFCXH731"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./afrispeech_large_A100\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=32,\n",
        "    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=2000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=200,\n",
        "    eval_steps=200,\n",
        "    logging_steps=10,\n",
        "    report_to=[\"tensorboard\",\"wandb\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "a2b462f9-cb4e-4fdc-b328-ff99202bf770",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:42:00.359991Z",
          "iopub.execute_input": "2023-05-30T15:42:00.361976Z",
          "iopub.status.idle": "2023-05-30T15:42:00.374472Z",
          "shell.execute_reply.started": "2023-05-30T15:42:00.361928Z",
          "shell.execute_reply": "2023-05-30T15:42:00.373726Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "a2b462f9-cb4e-4fdc-b328-ff99202bf770"
    },
    {
      "cell_type": "code",
      "source": [
        "# We reserve the first 500 files for test\n",
        "val_dataset = dataset['train'].take(500)\n",
        "train_dataset = dataset['train'].skip(500)"
      ],
      "metadata": {
        "id": "47a01258-41fa-4c16-9159-7a0b0553f0b8",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:42:00.619606Z",
          "iopub.execute_input": "2023-05-30T15:42:00.620259Z",
          "iopub.status.idle": "2023-05-30T15:42:00.625351Z",
          "shell.execute_reply.started": "2023-05-30T15:42:00.620223Z",
          "shell.execute_reply": "2023-05-30T15:42:00.624333Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "47a01258-41fa-4c16-9159-7a0b0553f0b8"
    },
    {
      "cell_type": "code",
      "source": [
        "# We shuffle the data\n",
        "train_dataset = train_dataset.shuffle()"
      ],
      "metadata": {
        "id": "kNHx6kzufNp3"
      },
      "id": "kNHx6kzufNp3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Training Configuration"
      ],
      "metadata": {
        "id": "zUdXhwVnJ5uE"
      },
      "id": "zUdXhwVnJ5uE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the final step, we define all the parameters related to training. For more detail on the training arguments, refer to the Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)."
      ],
      "metadata": {
        "id": "F1bXQY9AJ5uF"
      },
      "id": "F1bXQY9AJ5uF"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "839f4070-fac6-4d1b-aa6f-73b73e00a42b",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:42:10.918904Z",
          "iopub.execute_input": "2023-05-30T15:42:10.919380Z",
          "iopub.status.idle": "2023-05-30T15:42:10.957427Z",
          "shell.execute_reply.started": "2023-05-30T15:42:10.919341Z",
          "shell.execute_reply": "2023-05-30T15:42:10.956556Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "839f4070-fac6-4d1b-aa6f-73b73e00a42b"
    },
    {
      "cell_type": "code",
      "source": [
        "processor.save_pretrained(training_args.output_dir)"
      ],
      "metadata": {
        "id": "58dcd336-6420-4720-80b7-6684b7e83a30",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:42:20.043651Z",
          "iopub.execute_input": "2023-05-30T15:42:20.044099Z",
          "iopub.status.idle": "2023-05-30T15:42:20.251003Z",
          "shell.execute_reply.started": "2023-05-30T15:42:20.044065Z",
          "shell.execute_reply": "2023-05-30T15:42:20.249957Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "58dcd336-6420-4720-80b7-6684b7e83a30"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the GPU that you have, it is possible that you will encounter a CUDA `\"out-of-memory\"` error when you launch training. \n",
        "For `whisper-large-v2` we need GPUs with more memory like A100.\n",
        "To launch training, simply execute:"
      ],
      "metadata": {
        "id": "5a55168b-2f46-4678-afa0-ff22257ec06d"
      },
      "id": "5a55168b-2f46-4678-afa0-ff22257ec06d"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "a455f992-7adf-4863-8cce-89660ac774b6",
        "execution": {
          "iopub.status.busy": "2023-05-30T15:45:43.018668Z",
          "iopub.execute_input": "2023-05-30T15:45:43.019749Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "a455f992-7adf-4863-8cce-89660ac774b6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final winning model can be find at Huggingface Hub via this [link](https://huggingface.co/Seyfelislem/afripspeech_data_aug). The training metrics can be found `Training_metrics` [tab](https://huggingface.co/Seyfelislem/afripspeech_data_aug/tensorboard)."
      ],
      "metadata": {
        "id": "d1e_w8HVW49B"
      },
      "id": "d1e_w8HVW49B"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MJcSyALHgEKr"
      },
      "id": "MJcSyALHgEKr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}